{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "329564\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = defaultdict(lambda: {'topic': '', 'content': '', 'messages': []})\n",
    "\n",
    "for file_path in os.listdir(\"cleaned_intercom_messages\"):\n",
    "    csv = pd.read_csv(f\"cleaned_intercom_messages/{file_path}\")\n",
    "    csv = csv[csv['clean_body'].notna()]\n",
    "    bodies = csv[\"clean_body\"]\n",
    "    ids = zip(csv[\"key_intercomconversation\"], csv[\"key_iteration\"])\n",
    "    for id, body in zip(ids, bodies):\n",
    "        m = re.search(\" Selected category: (.+) Content: (.+) uid:\", body)\n",
    "        if m is not None:\n",
    "            data[id]['topic'] = m.group(1)\n",
    "            data[id]['content'] = m.group(2)\n",
    "        else:\n",
    "            data[id]['messages'].append(body)\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient=\"index\")\n",
    "print(len(df[df[\"topic\"] == '']))\n",
    "print(len(df[df[\"topic\"] != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {}\n",
    "\n",
    "#for index, row in test_df.iterrows():\n",
    "for index, row in df.iterrows():\n",
    "    msg = re.sub(r'{{[^}]*}}', '', \" \".join(row['messages']))\n",
    "    messages[index] = msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ingma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.extend([\"wo\", \"n't\", \"'m\", \"ca\", \"'ll\", \"'re\", \"'ve\", \"'d\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 329574\n",
      "3295 329574\n",
      "6590 329574\n",
      "9885 329574\n",
      "13180 329574\n",
      "16475 329574\n",
      "19770 329574\n",
      "23065 329574\n",
      "26360 329574\n",
      "29655 329574\n",
      "32950 329574\n",
      "36245 329574\n",
      "39540 329574\n",
      "42835 329574\n",
      "46130 329574\n",
      "49425 329574\n",
      "52720 329574\n",
      "56015 329574\n",
      "59310 329574\n",
      "62605 329574\n",
      "65900 329574\n",
      "69195 329574\n",
      "72490 329574\n",
      "75785 329574\n",
      "79080 329574\n",
      "82375 329574\n",
      "85670 329574\n",
      "88965 329574\n",
      "92260 329574\n",
      "95555 329574\n",
      "98850 329574\n",
      "102145 329574\n",
      "105440 329574\n",
      "108735 329574\n",
      "112030 329574\n",
      "115325 329574\n",
      "118620 329574\n",
      "121915 329574\n",
      "125210 329574\n",
      "128505 329574\n",
      "131800 329574\n",
      "135095 329574\n",
      "138390 329574\n",
      "141685 329574\n",
      "144980 329574\n",
      "148275 329574\n",
      "151570 329574\n",
      "154865 329574\n",
      "158160 329574\n",
      "161455 329574\n",
      "164750 329574\n",
      "168045 329574\n",
      "171340 329574\n",
      "174635 329574\n",
      "177930 329574\n",
      "181225 329574\n",
      "184520 329574\n",
      "187815 329574\n",
      "191110 329574\n",
      "194405 329574\n",
      "197700 329574\n",
      "200995 329574\n",
      "204290 329574\n",
      "207585 329574\n",
      "210880 329574\n",
      "214175 329574\n",
      "217470 329574\n",
      "220765 329574\n",
      "224060 329574\n",
      "227355 329574\n",
      "230650 329574\n",
      "233945 329574\n",
      "237240 329574\n",
      "240535 329574\n",
      "243830 329574\n",
      "247125 329574\n",
      "250420 329574\n",
      "253715 329574\n",
      "257010 329574\n",
      "260305 329574\n",
      "263600 329574\n",
      "266895 329574\n",
      "270190 329574\n",
      "273485 329574\n",
      "276780 329574\n",
      "280075 329574\n",
      "283370 329574\n",
      "286665 329574\n",
      "289960 329574\n",
      "293255 329574\n",
      "296550 329574\n",
      "299845 329574\n",
      "303140 329574\n",
      "306435 329574\n",
      "309730 329574\n",
      "313025 329574\n",
      "316320 329574\n",
      "319615 329574\n",
      "322910 329574\n",
      "326205 329574\n",
      "329500 329574\n"
     ]
    }
   ],
   "source": [
    "#words = word_tokenize(messages[13850823877])\n",
    "#words = messages[13850823877].split(\" \")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = {}\n",
    "stop_words = set(stop_words)\n",
    "length = len(messages)\n",
    "for i, (key, value) in enumerate(messages.items()):\n",
    "    words = word_tokenize(value)\n",
    "    lem = []\n",
    "    if i % (length // 100) == 0:\n",
    "        print(i, length)\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if len(lemma) > 1:\n",
    "            lem.append(lemma.lower())\n",
    "    lem = [w for w in lem if not w in stop_words]\n",
    "    lemmas[key] = lem\n",
    "    \n",
    "#print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lems = []\n",
    "\n",
    "for key in lemmas:\n",
    "    all_lems.extend(lemmas[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counter = Counter(all_lems).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_from_freq = [\"pipedrive\", \"please\", \"help\", \"know\", \"let\", \"see\", \"'s\", \"need\", \"would\", \"question\", \"``\", \"day\", \"get\", \"one\", \"possible\", \"feel\", \"wa\", \"''\", \"ha\", \"want\", \"like\", \"thank\", \"already\", \"yes\", \"ok\", \"done\", \"still\", \"great\", \"could\", \"soon\", \"also\", \"anything\", \"well\", \"right\", \"sure\", \"um\", \"else\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_from_freq = set(stop_words_from_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lemmas = dict()\n",
    "\n",
    "for key in lemmas:\n",
    "    new_lemmas[\"_\".join(map(str, key))] = [word for word in lemmas[key] if word not in stop_words_from_freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"new_lems.json\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    f.write(json.dumps(new_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
